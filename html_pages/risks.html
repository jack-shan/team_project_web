<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Group 100 Website</title>
    <link rel="stylesheet" href="../css/stylesheet.css">
  </head>

  <body>

    <div class="container">

      <div class="header">

        <h1>Group 100 Website</h1> 

      </div> <!-- close header div -->

      <div class="nav">

        <ul>
          <li><a href="../index.html">Home</a></li>
          <li><a href="../html_pages/topic.html">Topic</a></li>
          <li><a href="../html_pages/opportunities.html">Opportunities</a></li>
          <li><a href="../html_pages/risks.html">Risks</a></li>
          <li><a href="../html_pages/choices.html">Choices</a></li>
          <li><a href="../html_pages/meeting_minutes.html">Meeting Minutes</a></li>
          <li><a href="../html_pages/sources.html">Sources</a></li>
        </ul>

      </div> <!-- close nav div -->

      <div class="main">
        
        <h2>Risks</h2>
        <br>

        <p>
          As with any technological innovation, AI Robotic Companions for Seniors (ARCFS) have their own risk that comes with developing, implementing, and deploying such products.
        </p>

        <p>
          One major ethical risk that is becoming an issue in the present and future applications of ARCFS is the loss of human interactions for users of companions.
          In the past companions did not have AI integration and as a result, followed pre-programed instructions, as opposed to AI's dynamic responses.
          Throughout our research we all discovered, just how critical social interactions are for humans, and found one article that conducted an experiment, depriving
          test subject of social interactions for 10 hours, then tested them to see how much they craved social interaction over the course of the experiment. They found
          that humans need for social interaction can be equivalent to our need for food and water. “Thus, both forms of abstinence evoked craving for the specifically
          deprived need, along with general discomfort and decreased happiness.” (Tomova et al., 2020).  
          Several of the academic journal articles each group member found mentioned this as a risk factor for using ARCFS'. One article published in the US National library
          of Medicine discussed how “The concern about avatars replacing human contact relies on the assumption that interacting with human beings is more meaningful, stimulating,
          and comforting than interacting with artificial companions.” (Portacolone et al., 2020). We found this to be a scarily true concept. Coupling a senior who has a biological
          need for social interaction and an ARCFS such as ElliQ that has been specifically developed such that it “interacts with you in a way that feels natural, effortless,
          and personalized.” (Robotics, 2022), could result in a senior that feels that interactions with an ARCFS are more stimulating than interacting with a human. 
          The article goes on to touch on the potential profit-driven companies that could use ARCFS as a tool to almost exploit seniors and their families “However,
          one serious ethical problem that persists, independently from users' preferences, is that these avatar services make money by keeping people using
          them, and companies are incentivized to keep people emotionally hooked.” (Portacolone et al., 2020). Greed has always been an ethical risk for technology,
          and this example is no different from the likes of subscription-based services such as Netflix which hook people with tv shows and movies. ElliQ is a subscription-based
          product and as a result could inadvertently be causing families to fork out money, to pay for a companion, which their loved one has become, in essence, addicted to.  
        </p>

        <p>
          Another major ethical risk is misinformation and subsequently information bias. With ARCFS's constantly scouring the internet for information to feed back
          to its elderly user, it is inevitable that eventually false or biased information will be returned. One example of this occurred in 2021, when Amazons Alexa AI
          assistant instructed a 10-year-old girl to touch a coin to an exposed plug. An action that would've certainly seen her electrocuted. (Shead, 2021). Thankfully,
          her mother stopped her, but for a senior living alone, an equally dangerous task, could result in serious injury or even death. Our research and personal experience
          have shown us that society is becoming increasingly over-reliant on AI, with children and even adults using it to write essays and reports, which are often riddled with mistakes.
          “Adults too are increasingly relying on artificial intelligence for help with a wide range of daily tasks and social interactions, even though experts—including AI
          creators—have warned that chatbots are not only prone to errors but also “hallucinations.” In other words, chatbots make stuff up.” (Stover, 2023). Misinformation,
          especially for seniors, can be an extremely dangerous thing, leading them down a biased path towards ideals that could inevitably cause their own demise.
          One article explored the reasons for seniors' susceptibility to fake news. It found that the main reasons were cognitive decline, social changes, and digital illiteracy.
          Although overall, it was a combination of the three that contributed to the belief of misinformation “We argue that cognitive declines alone cannot explain older adults'
          engagement with fake news. Interventions in a “post-truth world” must also consider their shifting social goals and gaps in their digital literacy.” (Brashier & Schacter, 2020).
          These three reasons could result in ARCFS inadvertently turning into a tool that helps to spread misinformation, either through directly inaccurate sources, or becoming an
          echo chamber for seniors opinions, propagating unsubstantiated information. The spread of misinformation would likely lead to the further propagation of bias in a user's
          opinions. Although bias can be hardcoded into ARCFS from the start through the data the AI model is trained on. A publication written by the US National Institute of Standards
          and Technology, found that “human and systemic institutional and societal factors are significant sources of AI bias as well, and are currently overlooked” (Reva Schwartz, 2023). 
          We found this of particular interest as it helped us to understand that, no only was it data that contributed to AI bias, but the conscious or subconscious bias in
          the developers that also contributed to potential bias. In essence the work of an AI developer is a sort of reflection, and for and AI that has been fed training data
          that excludes certain information. This is a large, but often unseen risk, as it has the ability to severely affect the most vulnerable in our society. 
        </p>

        <p>
          Another major ethical risk is that of privacy. In an increasingly interconnected digital world, more and more data points are being scooped up by both major corporations and hackers. 
          Because ARCFS collects and disseminates incredibly large amounts of data, they are a perfect target for hacking. Our research has led us to discover several ways data can be
          exfiltrated from ARCF, including using its sensors. 
          To cybercriminals, personal data is like gold dust, and it is up to the companies that produce these ARCFS to protect your personal data from these criminals. Unfortunately,
          it is a constant battle and, on some days, the criminals win. Data breaches are becoming an all-too-common occurrence in our lives, and numerous papers we read, mentioned how
          big of an issue cybersecurity is becoming especially with the rise of AI. 
          One-way cyber criminals can glean data, is through the robot's own LiDAR sensors. Some ARCFS such as the LOVOT may utilize LiDAR sensors for navigation and
          a recent study found that the sensors could be used as an attack vector to eavesdrop on unsuspecting targets. The technique is called LidarPhone and was developed
          by a team of researchers from Singapore and the US. The technique works to “detect the minute vibrations induced on objects that are near audio sources, and extract
          meaningful signals from inherently noisy raw lidar returns.” (Sami et al., 2020). Such an attack would be undetectable by seniors and could result in financial data,
          such as passwords and account numbers being collected then later exploited by criminals. 
        </p>

      </div> <!-- close main div -->

      <div class="footer">

        <p id="text">Group 100 2024</p>

      </div> <!-- close footer div -->

    </div> <!-- close container div -->

  </body>

</html>